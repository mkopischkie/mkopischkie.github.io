---
title: "NN: Conclusion"
permalink: /nn_conclusion/
layout: single
classes: wide
---

The goal of the project was to predict GHS hazards from chemical compounds with the SMILE representation. A large gap exists if the government were posed with a synthesized biological threat. The function of the compound would take days to fully understand. With a tool to predict fuction, we can guess the GHS hazards and toxicity for each unique compound. The usefulness of this predictions extends beyond the government and into drug discovery as well. Many technologies exist for novel drug discovery and protein design using Artificial Intelligence (AI), but the function is not always known. Oftentimes, compounds are outputted from a model but require wetlab testing to determine the function. Using a neural network to predict the hazards of drugs and proteins on top of an AI-generation, we can determine the best compounds to test in a wetlab, saving time and money. 

The data used in my model came from a PubChem BioAssay API where I extracted the SMILE compound representations and the GHS hazards. I mapped each character in the SMILE vocabulary to an integer and padded the sequences so that they could be fed to a Long-Short Term Memory (LSTM) Network. After extensive cleaning and label replacement, I ran the data through my model to predict the hazards from the chemical compound representations. Although the model did not perform well, there are many reasons that may be that are listed under the results tab. However, initial exploration is vital for continued research. 

This initial research leaves many avenues to be explored. Firstly, further experimentation and hyperparamter tuning needs to be done. Adding more layers to the model and increasing the size of the drop out layer may help improve the validation accuracy and reduce overfitting. Secondly, I chose to represent the SMILE characters as integers, but one-hot-encoding could also work and needs to be explored. Thirdly, the embedding may not be working effectively in this model. I used the embedding from Keras, but there might be another approach that works better to embed integers rather than words. Making the embedding larger or smaller may have an impact on the validation accuracy. 

I mentioned earlier that integrating a symbolic or logic layer to the model may help it learn faster and better. This implies taking a neurosymbolic approach where the neural network learns patterns from data, but is guided by logical rules. For example, the aldehyde chemical group can be harmful to humans an animals and often have GHS hazard classifications including skin irritants and aquatic toxicity. If the model is already familiar and governed by generic GHS classifications for common chemical groups, it will reduce the learning time and improve the strength of the parameters. There are many methods to do this including diffusion models, score-based models, Logic Neural Networks (LNN), and some types of autoencoders. Aside from increasing the validation accuracy for prediction, a neurosymbolic approach can further expand this project. 

We already have tens-of-thousands of easily accessible chemical compounds for training, and with further compute resources, a neurosymbolic approach could generate additional compounds. Incorporating common chemistry and chemical geometry rules, an autoencoder or diffusion model can generate novel compounds by learning a denoising pattern. In other words, it learns how to make a given sample noisy and the equations used to denoise so that new compounds are generated. This would work in conjunction with the GHS hazard predictions. In essence, I hope to expand upon my project, not only to make the current methods more accurate, but to use a neurosymbolic approach to generate novel compounds to predict those functions as well. The drug discovery and prediction space will only grow larger as more advancements are made in AI and Machine Learning (ML). My project serves as a great introduction into this space with the potential to continue growing.  


